{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIGDalQn5GT4C756p9xXGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skovz99/DNATokenizer-2/blob/main/DNA_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz2t9JRtcYh1",
        "outputId": "c50a4c9c-f283-4196-b75f-d831f692c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a DNA transformer tokenizer part that takes a 3 nucleotide window as a single string for tokenization\n",
        "# first thing to do, convert the DNA sequence into strings of 3 in length\n",
        "DNAseq = 'ATGATGGGATCGGCGCTAGCTAGCTAGCTGGGTCAT'\n",
        "\n",
        "# define a function to separate the DNA sequence (all will be much longer than this example above) into equal parts of a k value prior to separating into three\n",
        "def equal_parts(DNA, part_size):\n",
        "  length = len(DNAseq)\n",
        "  parts = [DNA[i:i + part_size] for i in range(0, length, part_size)]\n",
        "  return parts\n",
        "\n",
        "# define a function to create spaces between the nucleotide windows in the DNA sequence\n",
        "def string(DNA, k):\n",
        "  result = []\n",
        "  for i in range(0, len(DNA), k):\n",
        "    chunk = \"\".join(DNA[i:i+3])\n",
        "    result.append(chunk)\n",
        "  return result\n",
        "\n",
        "three_string = string(DNAseq, k = 3)\n",
        "\n",
        "# once the DNA sequence has been split into 3 nucleotide wide strings define a function that determines the length of the vocabulary\n",
        "def vocab(initial_DNA, token_size):\n",
        "  DNA_length = len(initial_DNA)\n",
        "  unique_characters = len(list(set(initial_DNA)))\n",
        "  possible_tokens = unique_characters ** token_size\n",
        "  return possible_tokens\n",
        "\n",
        "vocabulary = vocab(DNAseq, 3)\n",
        "\n",
        "# create a vocabulary and tokenize the DNAseq, BOS = 0 and EOS = 1, no padding and no unknown because all sequences input into transformer are same size and no unknown nucleotides in initial DNA sequence\n",
        "# when the value of the size of nucleotide window is changed, this function will have to be updated to include either more or less for loops for the letters\n",
        "def combo_library(DNA, token_size, three_string, vocabulary):\n",
        "  uni = list(set(DNA))\n",
        "  combos = []\n",
        "  for letter1 in uni:\n",
        "    for letter2 in uni:\n",
        "      for letter3 in uni:\n",
        "        combination = letter1 + letter2 + letter3\n",
        "        combos.append(combination)\n",
        "  combo_tokens = [num for num in range(2, 66)]\n",
        "  token_assignment = list(zip(combos, combo_tokens))\n",
        "  result_tokens = []\n",
        "  for s in three_string:\n",
        "    for combo, token in token_assignment:\n",
        "      if s == combo:\n",
        "        result_tokens.append(token)\n",
        "        break\n",
        "      else:\n",
        "        result_tokens.append(vocabulary + 2)\n",
        "  result_tokens = [ele for ele in result_tokens if ele != (vocabulary + 2)]\n",
        "  result_tokens.insert(0, 0)\n",
        "  result_tokens.append(1)\n",
        "  return result_tokens\n",
        "\n",
        "DNA_tokens = combo_library(DNAseq, 3, three_string, vocabulary) # token size should be the same size as nucleotide window\n",
        "\n",
        "# convert the DNA tokens to a pytorch tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def tensor_tokens(DNA_tokens):\n",
        "  DNA_tensor = torch.tensor(DNA_tokens, dtype=torch.long)\n",
        "  return DNA_tensor\n",
        "\n",
        "DNA_tensor = tensor_tokens(DNA_tokens)\n",
        "\n",
        "# create embeddings for the DNA tensor\n",
        "def DNA_embeddings(dimension, vocabulary, DNA_tensor):\n",
        "  embedding_layer = nn.Embedding(vocabulary, dimension)\n",
        "  embedded_output = embedding_layer(DNA_tensor)\n",
        "  return embedded_output\n",
        "\n",
        "DNA_embed = DNA_embeddings(100, vocabulary, DNA_tensor)"
      ],
      "metadata": {
        "id": "oB0d0mbETNqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}